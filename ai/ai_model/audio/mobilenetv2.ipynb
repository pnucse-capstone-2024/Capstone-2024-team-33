{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import pandas as pd\n",
    "\n",
    "import tarfile\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from sklearn.metrics import f1_score\n",
    "import wandb\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(threshold=torch.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    SEED = 42\n",
    "    \n",
    "    CLASSIFIER_LR = 3e-4\n",
    "    \n",
    "    EPOCH = 200\n",
    "    BATCH_SIZE = 8\n",
    "    TARGET_SIZE = (128, 128)  \n",
    "\n",
    "    SR = 44100\n",
    "    N_MEL = 128\n",
    "    DURATION = 0.1\n",
    "    #NUM_AUGMENTATIONS = 10\n",
    "    N_FFT = 2048\n",
    "    HOP_LENGTH = 32\n",
    "\n",
    "    NOISE_DIR = 'background_noises'\n",
    "    DATA_DIR = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"audio_fall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CONFIG.SEED) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.classes = os.listdir(root_dir)\n",
    "        self.duration = 5.0\n",
    "\n",
    "        print(self.classes)\n",
    "\n",
    "        self.audio_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        for label, cls in enumerate(self.classes):\n",
    "            cls_path = os.path.join(root_dir, cls)\n",
    "            audio_files = os.listdir(cls_path)\n",
    "            for audio_file in tqdm(audio_files):\n",
    "                audio_path = os.path.join(cls_path, audio_file)\n",
    "                self.audio_paths.append(audio_path)\n",
    "                self.labels.append(label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_path = self.audio_paths[idx]\n",
    "        \n",
    "\n",
    "        y, sr = librosa.load(audio_path, sr=CONFIG.SR)\n",
    "\n",
    "        duration = librosa.get_duration(y=y, sr=sr)\n",
    "        num_samples_to_keep = int(self.duration * sr)\n",
    "    \n",
    "        if duration > self.duration:\n",
    "            y = y[:num_samples_to_keep]\n",
    "            duration = self.duration\n",
    "\n",
    "        else:\n",
    "        # 오디오가 5초보다 짧으면 패딩 추가\n",
    "            num_samples_to_pad = num_samples_to_keep - len(y)\n",
    "            y = np.pad(y, (0, num_samples_to_pad), mode='constant')\n",
    "        \n",
    "\n",
    "        S = librosa.feature.mfcc(\n",
    "            y=y, \n",
    "            sr=CONFIG.SR,\n",
    "            n_mfcc=40\n",
    "        )\n",
    "\n",
    "        S = cv2.resize(S, (224, 224))\n",
    "        \n",
    "\n",
    "        S = torch.FloatTensor(S)\n",
    "        S = S.unsqueeze(0)\n",
    "\n",
    "        \n",
    "\n",
    "        label = self.labels[idx]\n",
    "        label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "        return S, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset('./audio/train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset 및 타겟 정의\n",
    "targets = train_dataset.labels\n",
    "\n",
    "# StratifiedShuffleSplit 사용\n",
    "stratified_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=CONFIG.SEED)\n",
    "\n",
    "# StratifiedShuffleSplit은 인덱스를 반환하므로 이를 활용해 train/val 인덱스 분리\n",
    "for train_idx, val_idx in stratified_split.split(train_dataset, targets):\n",
    "    train_dataset_split = Subset(train_dataset, train_idx)\n",
    "    val_dataset_split = Subset(train_dataset, val_idx)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_loader = DataLoader(train_dataset_split, batch_size=CONFIG.BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset_split, batch_size=CONFIG.BATCH_SIZE, shuffle=False, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train Loader Data Count: {len(train_loader.dataset)}')\n",
    "print(f'Validation Loader Data Count: {len(val_loader.dataset)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset('./audio/test')\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=CONFIG.BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenetv2 = models.mobilenet_v2(pretrained=True)\n",
    "mobilenetv2.features[0] = nn.Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
    "mobilenetv2.classifier[1] = nn.Linear(in_features=1280, out_features=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomClassifier(nn.Module):\n",
    "    def __init__(self, backbone):\n",
    "        super(CustomClassifier, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, delta=0.0, mode='min', verbose=True):\n",
    "        \"\"\"\n",
    "        patience (int): loss or score가 개선된 후 기다리는 기간. default: 3\n",
    "        delta  (float): 개선시 인정되는 최소 변화 수치. default: 0.0\n",
    "        mode     (str): 개선시 최소/최대값 기준 선정('min' or 'max'). default: 'min'.\n",
    "        verbose (bool): 메시지 출력. default: True\n",
    "        \"\"\"\n",
    "        self.early_stop = False\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        \n",
    "        self.best_score = np.Inf if mode == 'min' else 0\n",
    "        self.mode = mode\n",
    "        self.delta = delta\n",
    "        \n",
    "\n",
    "    def __call__(self, score):\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        elif self.mode == 'min':\n",
    "            if score < (self.best_score - self.delta):\n",
    "                self.counter = 0\n",
    "                self.best_score = score\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Update) Best Score: {self.best_score:.5f}')\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Patience) {self.counter}/{self.patience}, ' \\\n",
    "                          f'Best: {self.best_score:.5f}' \\\n",
    "                          f', Current: {score:.5f}, Delta: {np.abs(self.best_score - score):.5f}')\n",
    "                \n",
    "        elif self.mode == 'max':\n",
    "            if score > (self.best_score + self.delta):\n",
    "                self.counter = 0\n",
    "                self.best_score = score\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Update) Best Score: {self.best_score:.5f}')\n",
    "            else:\n",
    "                self.counter += 1\n",
    "                if self.verbose:\n",
    "                    print(f'[EarlyStopping] (Patience) {self.counter}/{self.patience}, ' \\\n",
    "                          f'Best: {self.best_score:.5f}' \\\n",
    "                          f', Current: {score:.5f}, Delta: {np.abs(self.best_score - score):.5f}')\n",
    "                \n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            if self.verbose:\n",
    "                print(f'[EarlyStop Triggered] Best Score: {self.best_score:.5f}')\n",
    "            # Early Stop\n",
    "            self.early_stop = True\n",
    "        else:\n",
    "            # Continue\n",
    "            self.early_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    \n",
    "    for epoch in range(1, CONFIG.EPOCH):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for videos, labels in tqdm(iter(train_loader)):\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(videos)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val F1 : [{_val_score:.5f}]')\n",
    "        \n",
    "        wandb.log({\n",
    "            'epoch': epoch,\n",
    "            'train_loss': _train_loss,\n",
    "            'val_loss': _val_loss,\n",
    "            'val_f1': _val_score\n",
    "        })\n",
    "\n",
    "        es(_val_loss)\n",
    "\n",
    "        if es.early_stop:\n",
    "            print(\"Early Stopping\")\n",
    "            break\n",
    "    torch.save(model.state_dict(),  'audio_mb_sq.pt')\n",
    "        \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    preds, trues = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for videos, labels in tqdm(iter(val_loader)):\n",
    "            videos = videos.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            logit = model(videos)\n",
    "            \n",
    "            loss = criterion(logit, labels)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            preds += logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            trues += labels.detach().cpu().numpy().tolist()\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "    \n",
    "    _val_score = f1_score(trues, preds, average='weighted')\n",
    "    return _val_loss, _val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifer_model = CustomClassifier(backbone=mobilenetv2)\n",
    "classifier_optimizer = torch.optim.AdamW(params=classifer_model.parameters(), lr=CONFIG.CLASSIFIER_LR)\n",
    "classifier_scheduler = torch.optim.lr_scheduler.LambdaLR(classifier_optimizer, lr_lambda = lambda epoch: 1.0 ** CONFIG.EPOCH)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(patience=10, delta=0.0, mode='min', verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(classifer_model, classifier_optimizer, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_classifier_model = CustomClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_classifier_model.load_state_dict(torch.load('./audio_mb_sq.pt'))\n",
    "ckpt_classifier_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(best_classifier_model, test_loader, device):\n",
    "\n",
    "    best_classifier_model.to(device)\n",
    "    best_classifier_model.eval()\n",
    "\n",
    "    preds, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for S, label in tqdm(iter(test_loader)):\n",
    "\n",
    "            S = S.float().to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            classifier_output = best_classifier_model(S)\n",
    "\n",
    "\n",
    "\n",
    "            preds += classifier_output.argmax(1).detach().cpu().numpy().tolist()\n",
    "            labels += label.detach().cpu().numpy().tolist()\n",
    "\n",
    "\n",
    "\n",
    "    print(preds[:10])\n",
    "    print(labels[:10])\n",
    "    f1 = f1_score(labels, preds, average='weighted')\n",
    "    print(f'Test F1 Score: {f1:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(ckpt_classifier_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_duration = 5.0\n",
    "y, sr = librosa.load('./audio/test/fall/10.낙상_317473_label.wav', sr=44100)\n",
    "duration = librosa.get_duration(y=y, sr=sr)\n",
    "        \n",
    "if duration > model_duration:\n",
    "    num_samples_to_keep = int(model_duration * sr)\n",
    "    y = y[:num_samples_to_keep]\n",
    "    duration = model_duration\n",
    "    \n",
    "S = librosa.feature.mfcc(\n",
    "    y=y, \n",
    "    sr=sr,\n",
    "    n_mfcc=40\n",
    ")\n",
    "\n",
    "S = cv2.resize(S, (224, 224))\n",
    "\n",
    "    \n",
    "\n",
    "S = torch.FloatTensor(S)\n",
    "S = S.unsqueeze(0)\n",
    "S = S.unsqueeze(0)\n",
    "print(S.shape)\n",
    "ckpt_classifier_model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    S = S.to(device)\n",
    "    logit = ckpt_classifier_model(S)\n",
    "    pred = logit.argmax(1).detach().cpu().numpy()\n",
    "\n",
    "print(f\"pred = {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "model_duration = 5.0\n",
    "y, sr = librosa.load('./audio/test/fall/10.낙상_317473_label.wav', sr=44100)\n",
    "duration = librosa.get_duration(y=y, sr=sr)\n",
    "num_samples_to_keep = int(model_duration * sr)\n",
    "\n",
    "if duration > model_duration:\n",
    "    y = y[:num_samples_to_keep]\n",
    "    duration = model_duration\n",
    "else:\n",
    "    # 오디오가 5초보다 짧으면 패딩 추가\n",
    "    num_samples_to_pad = num_samples_to_keep - len(y)\n",
    "    y = np.pad(y, (0, num_samples_to_pad), mode='constant')\n",
    "    \n",
    "# 파형 시각화\n",
    "plt.figure(figsize=(12, 8))\n",
    "librosa.display.waveshow(y, sr=sr)\n",
    "plt.title('Waveform')\n",
    "plt.xlabel('Time (s)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "model_duration = 5.0\n",
    "y, sr = librosa.load('./audio/test/fall/10.낙상_317473_label.wav', sr=44100)\n",
    "duration = librosa.get_duration(y=y, sr=sr)\n",
    "num_samples_to_keep = int(model_duration * sr)\n",
    "\n",
    "if duration > model_duration:\n",
    "    y = y[:num_samples_to_keep]\n",
    "    duration = model_duration\n",
    "else:\n",
    "    # 오디오가 5초보다 짧으면 패딩 추가\n",
    "    num_samples_to_pad = num_samples_to_keep - len(y)\n",
    "    y = np.pad(y, (0, num_samples_to_pad), mode='constant')\n",
    "    \n",
    "# 파형 시각화\n",
    "S = librosa.feature.mfcc(\n",
    "    y=y, \n",
    "    sr=sr,\n",
    "    n_mfcc=40\n",
    ")\n",
    "\n",
    "S = cv2.resize(S, (224, 224))\n",
    "\n",
    "\n",
    "\n",
    "plt.imsave('haha.png', S, cmap=\"magma\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./audio/test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
